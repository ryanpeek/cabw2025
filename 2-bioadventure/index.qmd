---
title: Bioassessment Data Adventure
engine: knitr
format: 
  live-html:
      toc: true
webr:
  render-df: gt-interactive
  packages:
    - dplyr
    - tidyr
    - ggplot2
    - janitor
  cell-options:
    min-lines: 6
    editor-font-scale: 0.8
    fig-width: 7
    fig-height: 4.3
    out-width: "100%"
editor_options: 
  chunk_output_type: console
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}


The general approach here can be to join tables (need to update site locations to include the county so as to avoid needing sf and tigris).

Join tables, summarize by county, find county with highest CSCI scores and county with lowest CSCI scores.

Generate a quick box plot comparison by county

Look at trend latitudinally 

Generate summary table of stations to monitor

Find counties with fewest stations and identify sampling gaps

## Import our Data

First off we want to import our data so we can use it. For this data adventure, let's import three pieces, the `CSCI` values, the `ASCI` values, and a corresponding latitude & longitude list of our stations. We can then integrate all these pieces.

::: {.callout-tip}

## ðŸ“£ Load Libraries
If you are following along in your own RStudio Project, make sure to load the libraries first!^[These libraries are loaded automagically behind the scenes in this webpage, which is why we don't load them below.]  
We will use the following in this tutorial:

```
library(dplyr)
library(tidyr)
library(janitor)
library(ggplot2)
```
:::


```{webr}

csci_url <- "https://raw.githubusercontent.com/ryanpeek/cabw2025/refs/heads/main/data/cscidat.csv"
cscidat <- read.csv(csci_url)
str(cscidat)

asci_url <- "https://raw.githubusercontent.com/ryanpeek/cabw2025/refs/heads/main/data/ascidat.csv"
ascidat <- read.csv(asci_url)
glimpse(ascidat)

latlons <- read.csv(file =  "https://raw.githubusercontent.com/ryanpeek/cabw2025/refs/heads/main/data/latlon.csv")

glimpse(latlons)


```

## Selecting and Filtering

A big part of working with data is figuring out how to  `select`, `filter`, `slice`,, and `summarize`our data into the pieces we want. The good news is each of these things is a function in the {dplyr} package, so we can walk through a few examples for each of these *verbs*.

### `select`

We `select` columns. There are many options we can use to select the columns we want. Here are a few options, but there are many! Remember, we need to have loaded our `dplyr` package first. 

::: {.callout-tip}
## ðŸ“£ To "Quote" or not to Quote

One difficult thing to be aware of in R is when and where we use quotation marks. For the most part, calling column names in dplyr does **Not* require quoting. However, if we are using specific values, these typically need to be "quoted". Stay tuned for more!

:::

```{webr}
# first, select some columns
sel_1 <- select(
  # first the data
  .data = cscidat,
  # next the columns we want
  SampleID_old, New_Lat, New_Long, CSCI) 

glimpse(sel_1)
```

Try selecting everything but a few columns. Let's select all *but* `CSCI` and `COMID`

```{webr}

sel_2 <- select(cscidat, -CSCI, -COMID)
names(sel_2)

# another way to write this is using "c(CSCI, COMID)"
# however we need to put the "-" in front 
sel_2 <- select(cscidat, -c(CSCI, COMID))

```

Or we can select a columns that start with `"New"`

```{webr}
# select columns that start with "New"
sel_3 <- select(cscidat, starts_with('New'))

# or same sort of thing
sel_3 <- select(cscidat, contains('New'))
head(sel_3)

```

### `filter`

We `filter` **rows**. After we select some columns, youâ€™ll probably want to remove observations that donâ€™t fit some criteria. For example, maybe you want to remove CSCI scores less than some threshold, or find stations above a certain latitude, or both.

```{webr}

# get CSCI scores greater than 0.79
filt_1 <- filter(cscidat, CSCI > 0.79)

summary(filt_1$CSCI)

# get CSCI scores above latitude 37N
filt_2 <- filter(cscidat, New_Lat > 37)
summary(filt_2$New_Lat)

```

We can chain these together. In `filter()`, there are many ways to do this, but be aware if we use a `,` to separate different filters, it is treated as **`and`**. So both arguments must be met. For an either / or filter, we'd want to use a vertical bar, `|` separating our arguments.

```{webr}

# get CSCI scores above latitude 37N
filt_3 <- filter(cscidat, New_Lat > 37, CSCI > 0.79)

select(filt_3, New_Lat, CSCI) |> summary()

```

::: {.callout-tip}
## ðŸ“£ Chaining or Piping (`%>%` or `|>`)

We can chain code together by using a pipe argument (`%>%` or `|>`). These can be viewed as passing whatever is coming from the left side, on to the right side. Think of this as doing an operation on an object, pass to the pipe (`|>`) **and then** doing another operation. 

So, above, we select columns from our filtered dataset, **and then** we summarize those columns.

:::

### `slice`

`slice` is another function in the `{dplyr}` package, which allows us to do additional filtering of data based on values or summarizing actions. Here we'll use it to find the top ten CSCI values, but it is a powerful function with many uses, and can be easily chained together with other functions like `filter` and `select`.

```{webr}

# get top 10 highest CSCI scores 
csci_top_10 <- slice_max(cscidat, order_by = CSCI, n = 10)

# now get bottom 10 and select only the Station Code and CSCI score
csci_bot_10 <- slice_min(cscidat, order_by=CSCI, n=10) |> 
  select(StationCode, CSCI)

```


## Renaming

Often times we need to rename our columns in order to make joining data easier, or for tables and plots that we want to share out. There are a few ways to rename columns in R.

We can use the `dplyr::rename()` function to specifically rename columns of interest. This requires the data, and a list of the columns we are interested in changing, formatted as: `new_name` = `old_name`.

```{webr}

# rename with dplyr
cscidat_lower <- rename(cscidat, 
              id = SampleID_old, 
              lat = New_Lat, 
              lon = New_Long)
names(cscidat_lower)
```

Another great tool is the `{janitor}` package. We can use a number of functions in janitor, but most commonly I use it to clean all the column names. 

```{webr}

# rename all columns with janitor
cscidat_janitor <- janitor::clean_names(cscidat)
names(cscidat_janitor)

# take a look at options in the clean_names() function
# how can we change the case to inital caps?
# hint: help("clean_names")

```


## Modifying and Summarizing Data

When we want to add, revise, or summarize our data, we need to use a few additional functions in `{dplyr}`. We can use these to create new columns, calculate new data based on existing data, or create summaries based on different groups within our data.

### `mutate` (adding columns)

Weâ€™ve seen how to `filter` observations (rows) and `select` columns of a data frame. In `{dplyr}`, `mutate` allows us to add new columns to an existing data frame. These can be vectors you are adding or based on expressions applied to existing columns. For instance, maybe we want to convert a numeric column into a categorical using some criteria or maybe we want to make a new column based on some arithmetic on some other columns.

```{webr}

# get observed taxa
mut_1 <- mutate(cscidat, observed = OE * E)

glimpse(mut_1)

# add a column for lo/hi csci scores
mut_2 <- mutate(mut_1, CSCIcat = ifelse(CSCI <= 0.79, 'lo', 'hi'))
glimpse(mut_2)

```

### `summarize` Data

A very powerful verb in the `{dplyr}` package is `summarize`. Note, when we use `mutate`, we get back the same data frame, but with whatever revision or addition we have made to the columns. With `summarize`, we will get a **new dataframe** based on the summary operation we have requested.

The key with `summarize` is there is typically some sort of grouping variable that we want to use to do our summary. For example, maybe we want to get the max and min CSCI score by the stream `COMID` (unique segment ID for every stream line). 

```{webr}
csci_maxmin_by_comid <- cscidat |> # using pipe here to pass data on
  group_by(COMID) |> 
  summarize(maxCSCI = max(CSCI, na.rm=TRUE),
            minCSCI = min(CSCI, na.rm=TRUE))
glimpse(csci_maxmin_by_comid)

```

What if we want to know which of these `COMIDs` actually had more than one value to calculate the `min` and `max`? We can use another summarizing shortcut called `tally`. It works the same way as `summarize` but is designed specifically for counting unique values, or getting the sum of those values based on a grouping value. 
```{webr}

comid_tally <- cscidat |> 
  group_by(COMID) |> 
  tally() |> 
  filter(n>1)

```

### `bind` Data

Sometimes when we don't need to worry about joining specifically, and we want to append or mash datasets together, we can use the **bind** functions. There are base options as well as `{dplyr}` options. Let's revisit the `slice` function we used above, and make a new dataframe that is just the *StationCode*, *CSCI*, and *Lat* and *Lon* for the top 10 and bottom 10 CSCI values. We can bind rows, or bind columns, depending on what we are trying to do.

```{webr}
# get top 10 highest CSCI scores 
csci_top_10 <- slice_max(cscidat, order_by = CSCI, n = 10) |>   select(StationCode, CSCI, New_Lat, New_Long)

# now get bottom 10
csci_bot_10 <- slice_min(cscidat, order_by=CSCI, n=10) |> 
    select(StationCode, CSCI, New_Lat, New_Long)

# now BIND!
csci_top_bottom <- bind_rows(csci_top_10, csci_bot_10)
csci_top_bottom

# if we had different columns in each 
# data frame, what would happen with our bind?

```

::: {.callout-tip}

## ðŸ“£ Binding data frames
Remember, a vector is essentially a *column* of a single type of data. And data frames have to have columns with the same number of rows. So when binding rows, they need to be the same data `class` in each column, and when binding columns, they must have the same number of rows as the data frame. 

:::


## Joining

Combining data is a common task of data wrangling. All joins require that each of the tables can be linked by some shared identifier or **_key_** columns. These are usually represented as columns that are unique for each observations. Our example datasets include the `StationID` or `SampleID` columns that represent unique identifiers of the station, sample date, and/or replicate.

The challenge with joins is that the two datasets may not represent the same observations for a given key. For example, you might have one table with all observations for every key, another with only some observations, or two tables with only a few shared keys. What you get back from a join will depend on whatâ€™s shared between tables, in addition to the type of join you use.

For our data, weâ€™ll be using an **`inner_join`** that combines datasets by shared keys (for an overview of the other types of joins, see [R4DS lesson here](https://r4ds.had.co.nz/relational-data.html#mutating-joins).

![Figures by Garrick Adenbuie, see [here for more](https://www.garrickadenbuie.com/project/tidyexplain/)](https://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/inner-join.gif)

We need to either rename the **key** columns of interest so they are identical, or we can specify them explicitly when joining. Let's show both options below:

```{webr}

## OPTION 1: CHANGE COLUMNS TO THE SAME "KEY" NAME ------

# rename to "id" so cscidat matches ascidat
csci_renamed <- rename(cscidat, id = SampleID_old)

# join
all_data <- inner_join(csci_renamed, ascidat, by = 'id')
glimpse(all_data) # note how many rows and columns
# we expect to have added any matching rows from ascidat

## OPTION 2: SPECIFY THE KEY COLUMNS FOR EACH IN JOIN

all_data <- inner_join(cscidat, ascidat, by=c("SampleID_old"="id"))
glimpse(all_data)

```

Let's look at these two approaches above, break down the differences, and talk about what happened. Joins are particularly important, but can be difficult to deal with.

We could join by multiple columns, so if we wanted to join by `id` and the `New_Lat` and `New_Long` fields, we could say `by=c("SampleID_old"="id", "New_Lat", "New_Long")`. 
Note, for columns with the exact same name, we don't need the `=` to define for each dataset. 

It is always important to bear in mind what type of join we are using, understand and inspect the results, and check that against expectations of what we think we should get back. The computer will only do what we tell it, for better or worse!

## Plotting with `ggplot2`

The entire workflow of data exploration is enhanced through looking at your data, whether youâ€™re exploring a dataset for the first time or creating publication-ready figures. Viewing your data provides insight into patterns that can help you explore different hypotheses. No analysis is complete without a solid graphic.

Weâ€™ll only introduce some of the core concepts behind the popular ggplot2 package. This package follows a strict philosophy known as the grammar of graphics that was designed to make thinking, reasoning, and communicating about graphs easier by following a few simple rules. Like building a sentence in speech (aka grammar), all graphs start with a foundational component that is used for building other graph pieces.

With ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = alldat) creates an empty base graph.

ggplot(data = alldat)
You complete your graph by adding one or more layers (aka geoms) to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. Ggplot2 comes with many geom functions that each add a different type of layer to a plot.

```{webr}


ggplot(data = alldat) +
  geom_point()

```
